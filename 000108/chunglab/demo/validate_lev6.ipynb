{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b6d156-cd4d-45b3-88ae-dc848489ba8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dandi.dandiapi import DandiAPIClient\n",
    "import json\n",
    "from urllib.parse import quote, unquote\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285bc4bf-c246-4729-b48b-4898bf1a93cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore\n",
    "import zarr\n",
    "import s3fs\n",
    "fs = s3fs.S3FileSystem(anon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af05694-f4f3-42df-b3c7-e4ce0991d889",
   "metadata": {},
   "source": [
    "## Define utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4fa9fd-79a3-493b-8d17-fd2d36456276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dashboard\n",
    "def assets_to_df(ds):\n",
    "    assets = list(ds.get_assets())\n",
    "    asset_info = []\n",
    "    for asset in assets:\n",
    "        path_parts = asset.path.split(\"/\")\n",
    "        sub = None\n",
    "        for val in path_parts[:-1]:\n",
    "            if val.startswith(\"sub-\"):\n",
    "                sub = val.split(\"sub-\")[1]\n",
    "        assetname = path_parts[-1]\n",
    "        info = dict([[val.split(\"-\")[0], \"-\".join(val.split(\"-\")[1:])]\n",
    "                     for val in assetname.split(\".\")[0].split(\"_\")\n",
    "                     if \"-\" in val])\n",
    "        if sub:\n",
    "            info[\"subdir\"] = sub\n",
    "        info[\"path\"] = asset.path\n",
    "        modality = None\n",
    "        if \"_\" in assetname and \"sub-\" in assetname:\n",
    "            path = \"sub-\".join(asset.path.split(\"sub-\")[1:])\n",
    "            if len(path.split(\"/\")) > 1:\n",
    "                modality = assetname.split(\"_\")[-1].split(\".\")[0]\n",
    "                info[\"modality\"] = modality\n",
    "        ext = \".\".join(assetname.split(\".\")[1:])\n",
    "        info[\"extension\"] = ext\n",
    "        info[\"modified\"] = asset.modified\n",
    "        asset_info.append(info)\n",
    "    df = pd.DataFrame(asset_info)\n",
    "    return df, assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d476ceb-1c11-4b87-b6c0-741d04b41fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(ds, subj, sample, stain, ses):\n",
    "    zarrs = list(ds.get_assets_by_glob(f\"*{subj}/ses-{ses}/*_sample-{sample}_stain-{stain}_run-1*.ome.zarr\"))\n",
    "\n",
    "    sources = [f\"dandiarchive/zarr/{val.get_content_url(regex='s3').split('/')[-2]}/\" \n",
    "               for val in sorted(zarrs, key=lambda x: int(x.path.split(\"_chunk-\")[1].split(\"_\")[0]))]\n",
    "    return sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc827635-10cb-4e19-b397-7e4a0a073e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking basic things for arrays from a specific level \n",
    "def arrays3D_chunks_check(url_list, level=6):\n",
    "    arr_chunks = []\n",
    "    arr_errors = {\"Zeros\": [], \"ClientError\": [], f\"No level {level}\": []}\n",
    "    for ii, url in enumerate(url_list):\n",
    "        store_chunk = s3fs.S3Map(root=url, s3=fs, check=False)\n",
    "        try:\n",
    "            root = zarr.group(store=store_chunk)\n",
    "        except botocore.exceptions.ClientError:\n",
    "            arr_errors[\"ClientError\"].append(ii)\n",
    "        else:\n",
    "            try:\n",
    "                arr_chunks.append(root[f\"/{level}\"][0,0,:,:,:])\n",
    "            except KeyError:\n",
    "                arr_errors[f\"No level {level}\"].append(ii)\n",
    "            else:\n",
    "                if root[f\"/{level}\"][0,0,:,:,:].max() == 0:\n",
    "                    arr_errors[\"Zeros\"].append(ii)\n",
    "    \n",
    "    if len(set([arr.shape for arr in arr_chunks])) > 1:\n",
    "        arr_errors[\"Different shapes\"] = True\n",
    "    else:\n",
    "        arr_errors[\"Different shapes\"] = None\n",
    "    return arr_chunks, arr_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498254a9-e708-4ed6-84db-067ff4821646",
   "metadata": {},
   "source": [
    "## Getting assets for a specific dandiset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9355ab1b-7d38-4cf6-9526-3a3874a03edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dandiset = \"000108\"\n",
    "# dandiset = \"000026\"\n",
    "\n",
    "api = DandiAPIClient(\"https://api.dandiarchive.org/api\")\n",
    "ds = api.get_dandiset(dandiset)\n",
    "\n",
    "df, assets = assets_to_df(ds)\n",
    "\n",
    "# Make dandiset specific alterations to dataframe\n",
    "if dandiset == \"000108\":\n",
    "    remap = dict(calretinin='CR', npy='NPY')\n",
    "    def sample_to_int(x):\n",
    "        if isinstance(x, str) or not np.isnan(x):\n",
    "            return int(x.split('R')[0])\n",
    "        return x\n",
    "    df.stain = df.stain.apply(lambda x: remap[x] if x in remap else x)\n",
    "    #df['sample'] = df['sample'].apply(sample_to_int).astype(pd.Int64Dtype())\n",
    "if dandiset == \"000026\":\n",
    "    df = df[(df.path.str.contains(\"derivatives\") & \n",
    "             ((df.path.str.contains(\"EPIC\") == False) & \n",
    "              (df.path.str.contains(\"STER\") == False)))\n",
    "             == False]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb7189e-2492-463d-b176-224c61815ebe",
   "metadata": {},
   "source": [
    "## Running checks for level6 data for all subjects, samples, stains and sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db78ec3b-cec1-405c-ba41-79da361177d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in df.groupby(\"subdir\"):\n",
    "    sub = group[0]\n",
    "    print(f\"subject: {sub}\")\n",
    "    df_sub = df[(df[\"sub\"] == sub)]\n",
    "    df_sub_agg = df_sub.groupby(['sample', 'stain', 'ses'])[\"chunk\", \"path\"].agg(list)\n",
    "    df_sub_agg = pd.concat((df_sub_agg.index.to_frame(), df_sub_agg), axis=1)\n",
    "\n",
    "    df_sub_agg[\"errors\"] = None\n",
    "    samples = set(df_sub_agg[\"sample\"].tolist())\n",
    "    print(\"all samples: \", samples)\n",
    "    for ii, sample in enumerate(samples):\n",
    "        print(\"SAMPLE: \", ii, sample) \n",
    "        for stain in set(df_sub_agg.loc[sample][\"stain\"].tolist()):\n",
    "            for ses in df_sub_agg.loc[sample].loc[stain][\"ses\"].tolist():\n",
    "                url_list = get_url(ds, sub, sample, stain, ses)\n",
    "                _, arrays_lev6_errors  = arrays3D_chunks_check(url_list, level=6)\n",
    "                print(\"errors\", stain, arrays_lev6_errors)\n",
    "                df_sub_agg.loc[sample].loc[stain].loc[ses][\"errors\"] = arrays_lev6_errors\n",
    "\n",
    "\n",
    "    df_sub_agg.to_csv(f'dferr_sub-{sub}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
